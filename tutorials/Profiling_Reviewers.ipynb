{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Generating Stock Authors (Reviewer) LDA-based Profile\n",
    "\n",
    "Each author has her own research interests. By analyising her previous works (papers in this case), we can estimates her position in a **interest space**.\n",
    "\n",
    "In the LDA model implementation, a *interest space* is a n-dimensional space spanned by n-*topics*. Given a bag of words (BOW), a vector in the space can then be generating. Each coordinate of the vector is the *confidence* that the LDA model believes the BOW belongs to a particular *topic*.\n",
    "\n",
    "To get a vector of a paper, we preproccess the abstract of a paper and convert it to a BOW, then feed it to a trained LDA model. By summing up all the vectors of previous works of an author, we can get the author's position in the *interest space*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import gensim\n",
    "import nltk\n",
    "import json\n",
    "from gensim.corpora import BleiCorpus\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "## Helpers\n",
    "\n",
    "def save_pkl(target_object, filename):\n",
    "    with open(filename, \"wb\") as file:\n",
    "        pickle.dump(target_object, file)\n",
    "        \n",
    "def load_pkl(filename):\n",
    "    return pickle.load(open(filename, \"rb\"))\n",
    "\n",
    "def save_json(target_object, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(target_object, file)\n",
    "        \n",
    "def load_json(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33658"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con = sqlite3.connect(\"F:/FMR/data.sqlite\")\n",
    "\n",
    "db_documents = pd.read_sql_query(\"SELECT * from documents\", con, index_col=\"id\")\n",
    "db_authors = pd.read_sql_query(\"SELECT * from authors\", con)\n",
    "len(db_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>submission_date</th>\n",
       "      <th>cover_url</th>\n",
       "      <th>full_url</th>\n",
       "      <th>first_page</th>\n",
       "      <th>last_page</th>\n",
       "      <th>pages</th>\n",
       "      <th>document_type</th>\n",
       "      <th>type</th>\n",
       "      <th>article_id</th>\n",
       "      <th>context_key</th>\n",
       "      <th>label</th>\n",
       "      <th>publication_title</th>\n",
       "      <th>submission_path</th>\n",
       "      <th>journal_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Role-play and Use Case Cards for Requirements ...</td>\n",
       "      <td>&lt;p&gt;This paper presents a technique that uses r...</td>\n",
       "      <td>2006-01-01T00:00:00-08:00</td>\n",
       "      <td>2009-02-26T07:42:10-08:00</td>\n",
       "      <td>http://aisel.aisnet.org/acis2001/1</td>\n",
       "      <td>http://aisel.aisnet.org/cgi/viewcontent.cgi?ar...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>article</td>\n",
       "      <td>article</td>\n",
       "      <td>1001</td>\n",
       "      <td>742028</td>\n",
       "      <td>1</td>\n",
       "      <td>ACIS 2001 Proceedings</td>\n",
       "      <td>acis2001/1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Flexible Learning and Academic Performance in ...</td>\n",
       "      <td>&lt;p&gt;This research investigates the effectivenes...</td>\n",
       "      <td>2001-01-01T00:00:00-08:00</td>\n",
       "      <td>2009-02-26T22:04:53-08:00</td>\n",
       "      <td>http://aisel.aisnet.org/acis2001/10</td>\n",
       "      <td>http://aisel.aisnet.org/cgi/viewcontent.cgi?ar...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>article</td>\n",
       "      <td>article</td>\n",
       "      <td>1006</td>\n",
       "      <td>744077</td>\n",
       "      <td>10</td>\n",
       "      <td>ACIS 2001 Proceedings</td>\n",
       "      <td>acis2001/10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Proactive Metrics: A Framework for Managing IS...</td>\n",
       "      <td>&lt;p&gt;Managers of information systems development...</td>\n",
       "      <td>2001-01-01T00:00:00-08:00</td>\n",
       "      <td>2009-02-26T22:03:31-08:00</td>\n",
       "      <td>http://aisel.aisnet.org/acis2001/11</td>\n",
       "      <td>http://aisel.aisnet.org/cgi/viewcontent.cgi?ar...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>article</td>\n",
       "      <td>article</td>\n",
       "      <td>1005</td>\n",
       "      <td>744076</td>\n",
       "      <td>11</td>\n",
       "      <td>ACIS 2001 Proceedings</td>\n",
       "      <td>acis2001/11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reuse in Information Systems Development: Clas...</td>\n",
       "      <td>&lt;p&gt;There has been a trend in recent years towa...</td>\n",
       "      <td>2001-01-01T00:00:00-08:00</td>\n",
       "      <td>2009-02-26T22:02:29-08:00</td>\n",
       "      <td>http://aisel.aisnet.org/acis2001/12</td>\n",
       "      <td>http://aisel.aisnet.org/cgi/viewcontent.cgi?ar...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>article</td>\n",
       "      <td>article</td>\n",
       "      <td>1004</td>\n",
       "      <td>744075</td>\n",
       "      <td>12</td>\n",
       "      <td>ACIS 2001 Proceedings</td>\n",
       "      <td>acis2001/12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Improving Software Development: The Prescripti...</td>\n",
       "      <td>&lt;p&gt;We describe the Prescriptive Simplified Met...</td>\n",
       "      <td>2001-01-01T00:00:00-08:00</td>\n",
       "      <td>2009-02-26T22:01:24-08:00</td>\n",
       "      <td>http://aisel.aisnet.org/acis2001/13</td>\n",
       "      <td>http://aisel.aisnet.org/cgi/viewcontent.cgi?ar...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>article</td>\n",
       "      <td>article</td>\n",
       "      <td>1003</td>\n",
       "      <td>744074</td>\n",
       "      <td>13</td>\n",
       "      <td>ACIS 2001 Proceedings</td>\n",
       "      <td>acis2001/13</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "id                                                      \n",
       "1   Role-play and Use Case Cards for Requirements ...   \n",
       "2   Flexible Learning and Academic Performance in ...   \n",
       "3   Proactive Metrics: A Framework for Managing IS...   \n",
       "4   Reuse in Information Systems Development: Clas...   \n",
       "5   Improving Software Development: The Prescripti...   \n",
       "\n",
       "                                             abstract  \\\n",
       "id                                                      \n",
       "1   <p>This paper presents a technique that uses r...   \n",
       "2   <p>This research investigates the effectivenes...   \n",
       "3   <p>Managers of information systems development...   \n",
       "4   <p>There has been a trend in recent years towa...   \n",
       "5   <p>We describe the Prescriptive Simplified Met...   \n",
       "\n",
       "             publication_date            submission_date  \\\n",
       "id                                                         \n",
       "1   2006-01-01T00:00:00-08:00  2009-02-26T07:42:10-08:00   \n",
       "2   2001-01-01T00:00:00-08:00  2009-02-26T22:04:53-08:00   \n",
       "3   2001-01-01T00:00:00-08:00  2009-02-26T22:03:31-08:00   \n",
       "4   2001-01-01T00:00:00-08:00  2009-02-26T22:02:29-08:00   \n",
       "5   2001-01-01T00:00:00-08:00  2009-02-26T22:01:24-08:00   \n",
       "\n",
       "                              cover_url  \\\n",
       "id                                        \n",
       "1    http://aisel.aisnet.org/acis2001/1   \n",
       "2   http://aisel.aisnet.org/acis2001/10   \n",
       "3   http://aisel.aisnet.org/acis2001/11   \n",
       "4   http://aisel.aisnet.org/acis2001/12   \n",
       "5   http://aisel.aisnet.org/acis2001/13   \n",
       "\n",
       "                                             full_url first_page last_page  \\\n",
       "id                                                                           \n",
       "1   http://aisel.aisnet.org/cgi/viewcontent.cgi?ar...                        \n",
       "2   http://aisel.aisnet.org/cgi/viewcontent.cgi?ar...                        \n",
       "3   http://aisel.aisnet.org/cgi/viewcontent.cgi?ar...                        \n",
       "4   http://aisel.aisnet.org/cgi/viewcontent.cgi?ar...                        \n",
       "5   http://aisel.aisnet.org/cgi/viewcontent.cgi?ar...                        \n",
       "\n",
       "   pages document_type     type article_id context_key label  \\\n",
       "id                                                             \n",
       "1              article  article       1001      742028     1   \n",
       "2              article  article       1006      744077    10   \n",
       "3              article  article       1005      744076    11   \n",
       "4              article  article       1004      744075    12   \n",
       "5              article  article       1003      744074    13   \n",
       "\n",
       "        publication_title submission_path  journal_id  \n",
       "id                                                     \n",
       "1   ACIS 2001 Proceedings      acis2001/1           1  \n",
       "2   ACIS 2001 Proceedings     acis2001/10           2  \n",
       "3   ACIS 2001 Proceedings     acis2001/11           3  \n",
       "4   ACIS 2001 Proceedings     acis2001/12           4  \n",
       "5   ACIS 2001 Proceedings     acis2001/13           5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenised = load_json(\"lemmatized.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_en = load_pkl(\"non_en.list.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenised) == len(db_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LdaModel.load(\"aisnet_600_cleaned.ldamodel\")\n",
    "dictionary = Dictionary.load(\"aisnet_300_cleaned.ldamodel.dictionary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting (Predicting) Topics Distribution From Raw Text\n",
    "\n",
    "`predict` function will predict the topics distributions from a given raw text. The result is a pandas dataframe, with topics ids and confidence thereof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text2vec(text):\n",
    "    if text:\n",
    "        return dictionary.doc2bow(TextBlob(text.lower()).noun_phrases)\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "def tokenised2vec(tokenised):\n",
    "    if tokenised:\n",
    "        return dictionary.doc2bow(tokenised)\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "def predict(sometext):\n",
    "    vec = text2vec(sometext)\n",
    "    dtype = [('topic_id', int), ('confidence', float)]\n",
    "    topics = np.array(model[vec], dtype=dtype)\n",
    "    topics.sort(order=\"confidence\")\n",
    "#     for topic in topics[::-1]:\n",
    "#         print(\"--------\")\n",
    "#         print(topic[1], topic[0])\n",
    "#         print(model.print_topic(topic[0]))\n",
    "    return pd.DataFrame(topics)\n",
    "\n",
    "def predict_vec(vec):\n",
    "    dtype = [('topic_id', int), ('confidence', float)]\n",
    "    topics = np.array(model[tokenised2vec(vec)], dtype=dtype)\n",
    "    topics.sort(order=\"confidence\")\n",
    "#     for topic in topics[::-1]:\n",
    "#         print(\"--------\")\n",
    "#         print(topic[1], topic[0])\n",
    "#         print(model.print_topic(topic[0]))\n",
    "    return pd.DataFrame(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>167</td>\n",
       "      <td>0.286322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic_id  confidence\n",
       "0       167    0.286322"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"null values are interpreted as unknown value or inapplicable value. This paper proposes a new approach for solving the unknown value problems with Implicit Predicate (IP). The IP serves as a descriptor corresponding to a set of the unknown values, thereby expressing the semantics of them. In this paper, we demonstrate that the IP is capable of (1) enhancing the semantic expressiveness of the unknown values, (2) entering incomplete information into database and (3) exploiting the information and a variety of inference rules in database to reduce the uncertainties of the unknown values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.060*\"critical\" + 0.032*\"critical success factor\" + 0.025*\"digital native\" + 0.019*\"success factor\" + 0.016*\"trial\" + 0.010*\"personal information management\" + 0.010*\"reach\" + 0.010*\"layne\" + 0.009*\"use context\" + 0.008*\"bjrnandersen\"'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topic(167)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Author's Topic Vector\n",
    "\n",
    "The vector is a topic confidence vector for the author. The length of the vector should be the number of topics in the LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_author_vector(vec, doc_vec):\n",
    "    for topic_id, confidence in zip(doc_vec['topic_id'], doc_vec['confidence']):\n",
    "        vec[topic_id] += confidence\n",
    "    return vec\n",
    "\n",
    "def get_topic_in_list(model, topic_id):\n",
    "    return [term.strip().split('*') for term in model.print_topic(topic_id).split(\"+\")]\n",
    "\n",
    "def get_author_top_topics(author_id, top=10):\n",
    "    author = authors_lib[author_id]\n",
    "    top_topics = []\n",
    "    for topic_id, confidence in enumerate(author):\n",
    "        if confidence > 1:\n",
    "            top_topics.append([topic_id, (confidence - 1) * 100])\n",
    "    top_topics.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    return top_topics[:top]\n",
    "\n",
    "def get_topic_in_string(model, topic_id, top=5):\n",
    "    topic_list = get_topic_in_list(model, topic_id)\n",
    "    topic_string = \" / \".join([i[1] for i in topic_list][:top])\n",
    "    return topic_string\n",
    "\n",
    "def get_topics_in_string(model, topics, confidence=False):\n",
    "    if confidence:\n",
    "        topics_list = []\n",
    "        for topic in topics:\n",
    "            topic_map = {\n",
    "                \"topic_id\": topic[0],\n",
    "                \"string\": get_topic_in_string(model, topic[0]),\n",
    "                \"confidence\": topic[1]\n",
    "            }\n",
    "            topics_list.append(topic_map)\n",
    "    else:\n",
    "        topics_list = []\n",
    "        for topic_id in topics:\n",
    "            topic_map = {\n",
    "                \"topic_id\": topic_id,\n",
    "                \"string\": get_topic_in_string(model, topic_id),\n",
    "            }\n",
    "            topics_list.append(topic_map)\n",
    "    return topics_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a author, we first get all his previous papers in our database. For each paper we get, we generate a paper's vector. At last, the sum of all vectors will be the vector (aka the position) in the *interest space*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def profile_author(author_id, model_topics_num=None):\n",
    "    if not model_topics_num:\n",
    "        model_topics_num = model.num_topics\n",
    "    author_vec = np.array([1.0 for i in range(model_topics_num)])\n",
    "    # Initialize with 1s\n",
    "    paper_list = pd.read_sql_query(\"SELECT * FROM documents_authors WHERE authors_id=\" + str(author_id), con)['documents_id']\n",
    "    paper_list = [i for i in paper_list if i not in non_en]\n",
    "    # print(paper_list)\n",
    "    for paper_id in paper_list:\n",
    "        try:\n",
    "            abstract = db_documents.loc[paper_id][\"abstract\"]\n",
    "            vec = predict_vec(tokenised[paper_id -1])\n",
    "        except:\n",
    "            print(\"Error occurred on paper id \" + str(paper_id))\n",
    "            raise\n",
    "        author_vec = update_author_vector(author_vec, vec)\n",
    "    return list(author_vec) # to make it serializable by JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 283]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.02614883,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.02759921,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.01478242,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.09948292,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.21489484,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.09316398,  1.        ,  1.        ,  1.        ,\n",
       "        1.44264631,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.01513048,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.01562679,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.08548864,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.01420303,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.29817268,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.02064749,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.01190146,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.01655639,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.09039075,  1.        ,\n",
       "        1.        ,  1.        ,  1.06152949,  1.        ,  1.        ])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_author(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def profile_all_authors():\n",
    "    authors = {}\n",
    "    for author_id in db_authors['id']:\n",
    "        result = profile_author(author_id)\n",
    "        if len(result):\n",
    "            authors[str(author_id)] = result # JSON does not allow int to be the key\n",
    "        # print(\"Done: \", author_id)\n",
    "        # uncomment the above line to track the progress\n",
    "    return authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "authors_lib = profile_all_authors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(db_authors) == len(authors_lib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Library (aka Pool of Scholars)\n",
    "\n",
    "We will save our profiled authors in a JSON file. It will then used by our matching algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_json(authors_lib, \"aisnet_600_cleaned.authors.json\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
